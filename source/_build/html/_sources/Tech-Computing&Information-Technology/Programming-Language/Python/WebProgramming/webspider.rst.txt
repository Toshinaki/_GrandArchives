
Python Spider
==============

Introduction
--------------

什么是爬虫
~~~~~~~~~~~~~~

例子:

1. 搜索引擎
2. 数据分析
3. 咨询网站

web crawler/spider, 自动模拟网页请求抓取数据

- 通用爬虫: 搜索引擎抓取系统的重要组成部分, 主要用于将互联网上的网页下载到本地, 形成镜像备份
- 聚焦爬虫: 面向特定需求, 只抓取网页的指定部分

HTTP/HTTPS
~~~~~~~~~~~~~~

- HTTP: HyperText Transfer Protocol, 超文本传输协议, 用于发布和接收 HTML 网页; 服务器默认端口为 80
- HTTPS: HTTP over Secure Socket Layer, 在 HTTP 的基础上加入了 SSL 层; 服务器默认端口是 443

.. image:: imgs/HTTP.jpg
    :scale: 80%

request methods
^^^^^^^^^^^^^^^^^

- GET
- POST
- HEAD
- PUT
- DELETE
- TRACE
- OPTIONS
- CONNECT
- PATCH

常见请求头
^^^^^^^^^^^^^^

.. image:: imgs/http_request.jpg

.. list-table::
    :widths: auto
    :header-rows: 1
    :stub-columns: 0

    * - Host
      - 接收请求的服务器地址
    * - User-Agent
      - 发送请求的应用程序名称

        爬虫里需要伪造
    * - Content-Type
      - 也叫互联网媒体类型 (Internet Media Type) 或者 MIME 类型

        在 HTTP 协议消息头中用来表示具体请求中的媒体类型信息
    * - Referer
      - 表明当前请求是从哪个页面发过来的

        一般也用于反爬虫技术; 如果不是从指定页面过来的就不响应
    * - Cookie
      - 记录一些请求的状态

        例如爬虫访问需要登陆的网站就需要发送 cookie
    * - Connection
      - 指定与连接相关的属性, 如 Connection:Keep-Alive
    * - Accept
      - 请求报头域, 用于指定客户端可接受哪些类型的信息
    * - Accept-Charset
      - 通知服务端可以发送的编码格式
    * - Accept-Encoding
      - 通知服务端可以发送的数据压缩格式
    * - Accept-Language
      - 通知服务端可以发送的语言

.. admonition:: 例

    .. code-block::

        POST 　/index.php　HTTP/1.1 　　 请求行
        Host: localhost
        User-Agent: Mozilla/5.0 (Windows NT 5.1; rv:10.0.2) Gecko/20100101 Firefox/10.0.2　　请求头
        Accept: text/html,application/xhtml+xml,application/xml;q=0.9,/;q=0.8
        Accept-Language: zh-cn,zh;q=0.5
        Accept-Encoding: gzip, deflate
        Connection: keep-alive
        Referer: http://localhost/
        Content-Length: 25
        Content-Type: application/x-www-form-urlencoded
        　　空行
        username=aa&password=1234　　请求数据

Content-Type 和 POST 提交数据方式的关系:

.. list-table::
    :widths: auto
    :header-rows: 1
    :stub-columns: 0

    * - Content-Type
      - 提交数据的方式
    * - application/x-www-form-urlencoded
      - 表单数据
    * - multipart/form-data
      - 表单文件上传
    * - application/json
      - 序列化 JSON 数据
    * - text/xml
      - XML 数据

常见响应状态码
^^^^^^^^^^^^^^^^^

.. list-table::
    :widths: auto
    :header-rows: 1
    :stub-columns: 0

    * - 状态码
      - 说明
    * - 200
      - 响应成功
    * - 301
      - 永久重定向
    * - 302
      - 临时重定向; 如重定向到登陆页面
    * - 400
      - 请求的 URL 在服务器上找不到
    * - 403
      - 服务器拒绝访问, 权限不够
    * - 500
      - 服务器内部错误

常见响应头
^^^^^^^^^^^^^^^

- **Date**: 标识响应产生的时间
- **Last-Modified**: 指定资源的最后修改时间
- **Content-Encoding**: 指定响应内容的编码
- **Server**: 包含服务器的信息, 比如名称, 版本号等
- **Content-Type**: 文档类型, 指定返回的数据类型是什么, 如 text/html 代表返回 HTML 文档, application/x-javascript 则代表返回 JavaScript 文件, image/jpeg 则代表返回图片
- **Set-Cookie**: 设置 Cookies; 响应头中的 Set-Cookie 告诉浏览器需要将此内容放在 Cookies 中, 下次请求携带 Cookies 请求
- **Expires**: 指定响应的过期时间, 可以使代理服务器或浏览器将加载的内容更新到缓存中; 如果再次访问时, 就可以直接从缓存中加载, 降低服务器负载, 缩短加载时间

URI & URL
~~~~~~~~~~~

Uniform Resource Identifier, 统一资源标志符; Uniform Resource Locator, 统一资源定位符

URL 是 URI 的子集; URI 还包括一个子类叫作 **URN**, Universal Resource Name, 即统一资源名称; URN 只命名资源而不指定如何定位资源, 比如 `urn:isbn:0451450523` 指定了一本书的 ISBN, 可以唯一标识这本书, 但是没有指定到哪里定位这本书

**标准格式**:

`[schema]://[host]:[port]/[path]?[query]#[fragment]`

- schema: 传送协议
- 层级URL标记符号 (为[//],固定不变)
- authority: 访问资源需要的凭证信息 (可省略)
- host: 服务器, (通常为域名, 有时为IP地址)
- port: 端口号, (以数字方式表示, 若为默认值可省略)
- path: 路径, (以“/”字符区别路径中的每一个目录名称)
- query: 查询, (GET模式的窗体参数, 以“?”字符为起点, 每个参数以“&”隔开, 再以“=”分开参数名称与数据, 通常以UTF8的URL编码, 避开字符冲突的问题)
- fragment: 片段, 以 "#" 字符为起点

**完整格式**:

`[协议类型]://[访问资源需要的凭证信息]@[服务器地址]:[端口号]/[资源层级UNIX文件路径][文件名]?[查询]#[片段ID]`

`scheme:[//authority]path[?query][#fragment]`

Session & Cookies
~~~~~~~~~~~~~~~~~~~~~~~

HTTP 的 **无状态** 是指 HTTP 协议对事务处理没有记忆能力, 即服务器不知道客户端是什么状态

向服务器发送请求后, 服务器解析请求, 然后返回对应的响应; 服务器负责完成这个过程, 而且这个过程是完全独立的, 服务器不会记录前后状态的变化, 也就是缺少状态记录

这意味着如果后续需要处理前面的信息, 则必须重传, 这导致需要额外传递一些前面的重复请求, 才能获取后续响应

为了保持前后状态, 需要用到 session 和 cookies

- **Session** 在服务端保存用户的会话信息

    会话对象用来存储特定用户会话所需的属性及配置信息

    当用户在应用程序的 Web 页之间跳转时, 存储在会话对象中的变量将不会丢失

    服务器会自动创建会话对象; 当会话过期或被放弃后, 服务器将终止该会话

- **Cookies** 在客户端保存登录的凭证

    Cookies 指某些网站为了辨别用户身份, 进行会话跟踪而存储在用户本地终端上的数据

    .. image:: imgs/cookie.jpg
        :align:right

    1. 当客户端第一次请求服务器时, 服务器会返回一个响应头中带有 Set-Cookie 字段的响应给客户端, 用来标记是哪一个用户, 客户端浏览器会把 Cookies 保存起来
    2. 当浏览器下一次再请求该网站时, 浏览器会把此 Cookies 放到请求头一起提交给服务器, Cookies 携带了会话 ID 信息
    3. 服务器检查该 Cookies 即可找到对应的会话是什么, 然后再判断会话来以此来辨认用户状态

Cookies 属性
^^^^^^^^^^^^^^

- **Name**: 即该 Cookie 的名称； Cookie 一旦创建, 名称便不可更改
- **Value**: 即该 Cookie 的值; 如果值为 Unicode 字符, 需要为字符编码; 如果值为二进制数据, 则需要使用 BASE64 编码
- **Max Age**: 即该 Cookie 失效的时间, 单位秒, 也常和 Expires 一起使用, 通过它可以计算出其有效时间; Max Age 如果为正数, 则该 Cookie 在 Max Age 秒之后失效; 如果为负数, 则关闭浏览器时 Cookie 即失效, 浏览器也不会以任何形式保存该 Cookie
- **Path**: 即该 Cookie 的使用路径。如果设置为 `/path/`, 则只有路径为 `/path/` 的页面可以访问该 Cookie; 如果设置为 `/`, 则本域名下的所有页面都可以访问该 Cookie
- **Domain**: 即可以访问该 Cookie 的域名; 例如如果设置为 `.zhihu.com`, 则所有以 `zhihu.com` 结尾的域名都可以访问该 Cookie
- **Size**: 即此 Cookie 的大小
- **HTTP**: 即 Cookie 的 httponly 属性; 若此属性为 true, 则只有在 HTTP Headers 中会带有此 Cookie 的信息, 而不能通过 `document.cookie` 来访问此 Cookie
- **Secure**: 即该 Cookie 是否仅被使用安全协议传输; 安全协议有 HTTPS, SSL 等, 在网络上传输数据之前先将数据加密; 默认为 false

Proxy server
~~~~~~~~~~~~~~

为了避免同一个 IP 访问过于频繁, 使用代理伪装 IP

代理服务器, 代理网络用户去取得网络信息

分类
^^^^^^^

- 根据协议区分:

    - **FTP**: 主要用于访问 FTP 服务器, 一般有上传, 下载以及缓存功能, 端口一般为 21, 2121 等
    - **HTTP**: 主要用于访问网页, 一般有内容过滤和缓存功能, 端口一般为 80, 8080, 3128 等
    - **SSL/TLS**: 主要用于访问加密网站, 一般有 SSL 或 TLS 加密功能 (最高支持 128 位加密强度), 端口一般为 443
    - **RTSP**: 主要用于 Realplayer 访问 Real 流媒体服务器, 一般有缓存功能, 端口一般为 554
    - **Telnet**: 主要用于 telnet 远程控制 (黑客入侵计算机时常用于隐藏身份), 端口一般为 23
    - **POP3/SMTP**: 主要用于 POP3/SMTP 方式收发邮件, 一般有缓存功能, 端口一般为 110/25
    - **SOCKS**: 只是单纯传递数据包, 不关心具体协议和用法, 所以速度快很多, 一般有缓存功能, 端口一般为 1080; SOCKS 代理协议又分为 SOCKS4 和 SOCKS5, SOCKS4 协议只支持 TCP, 而 SOCKS5 协议支持 TCP 和 UDP, 还支持各种身份验证机制, 服务器端域名解析等

- 根据匿名程度区分:

    - **高度匿名代理**: 高度匿名代理会将数据包原封不动的转发, 在服务端看来就好像真的是一个普通客户端在访问, 而记录的 IP 是代理服务器的 IP
    - **普通匿名代理**: 普通匿名代理会在数据包上做一些改动, 服务端上有可能发现这是个代理服务器, 也有一定几率追查到客户端的真实 IP; 代理服务器通常会加入的 HTTP 头有 HTTP_VIA 和 HTTP_X_FORWARDED_FOR
    - **透明代理**: 透明代理不但改动了数据包, 还会告诉服务器客户端的真实 IP; 这种代理除了能用缓存技术提高浏览速度, 能用内容过滤提高安全性之外, 并无其他显著作用, 最常见的例子是内网中的硬件防火墙
    - **间谍代理**: 间谍代理指组织或个人创建的, 用于记录用户传输的数据, 然后进行研究, 监控等目的代理服务器

常见代理设置
^^^^^^^^^^^^^^^

- 使用网上的免费代理, 最好使用高匿代理, 使用前抓取下来筛选一下可用代理, 也可以进一步维护一个代理池
- 使用付费代理服务, 互联网上存在许多代理商, 可以付费使用, 质量比免费代理好很多
- ADSL 拨号, 拨一次号换一次 IP, 稳定性高, 也是一种比较有效的解决方案

Python 爬虫库
-----------------

urllib
~~~~~~~~

Python 内置的 HTTP 请求库, 包含 4 个模块:

- **request**: 最基本的 HTTP 请求模块, 可以用来模拟发送请求
- **error**: 异常处理模块, 如果出现请求错误, 可以捕获异常, 然后进行重试或其他操作以保证程序不会意外终止
- **parse**: 一个工具模块, 提供了许多 URL 处理方法, 比如拆分, 解析, 合并等
- **robotparser**: 主要用来识别网站的 robots.txt 文件, 然后判断哪些网站可以爬, 哪些网站不可以爬; 用得比较少

request
^^^^^^^^^^

urllib.request 模块提供了最基本的构造 HTTP 请求的方法, 利用它可以模拟浏览器的一个请求发起过程, 同时它还带有处理授权验证（authentication) , 重定向（redirection), 浏览器 Cookies 以及其他内容

urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None)
    打开 URL 或 `Request` 对象; urllib.request 模块使用 HTTP/1.1 并在 HTTP 头中包括了 Connection:close

    PARAMS:
        - url: URL 或 `Request` 对象
        - data: 发送给服务端的数据; 未指定则使用 GET, 否则使用 POST; 必须为 bytes 类型

            .. code-block:: python

                data = bytes(urllib.parse.urlencode({'word': 'hello'}), encoding='utf8')
                response = urllib.request.urlopen('http://httpbin.org/post', data=data)
                print(response.read())

        - 可选的 `timeout` 指定了阻塞操作 (如尝试连接) 的以秒为单位的 timeout; 只对 HTTP, HTTPS & FTP 连接有效; 默认为 `socket._GLOBAL_DEFAULT_TIMEOUT`; 超时则抛出 `urllib.error.URLError` 异常

            .. code-block:: python

                try:
                    response = urllib.request.urlopen('http://httpbin.org/get', timeout=0.1)
                except urllib.error.URLError as e:
                    if isinstance(e.reason, socket.timeout):
                        print('TIME OUT')

        - context: `ssl.SSLContext` 实例, 用于指定 SSL 设置
        - `cafile` & `capath` (optional): 指定了用于 HTTPS 请求的 CA certificates; `cafile` 包含了 CA certificates 的文件路径; `capath` 指向含有 hashed certificate 文件的路径
        - `cadefault`: 已弃用

    RETURN:
        返回 `http.client.HTTPResponse` 对象 (类文件句柄对象), 可用作 context manager; 属性和方法:

            - `status`: 状态码
            - `geturl()`: 返回资源的 URL, 可用于判断是否有重定向
            - `info()`: return the meta-information of the page, such as headers, in the form of an email.message_from_string() instance
            - `getheaders()`: 返回所有 headers
            - `getheader(str)`: 返回指定 header
            - `getcode()`: 返回 HTTP 状态码
            - `read(size)`
            - `readline()`
            - `readlines()`

urllib.request.Request(url, data=None, headers={}, origin_req_host=None, unverifiable=False, method=None)
    构造 `Reques` 对象

    PARAMS:
        - url: 请求的 URL
        - data: 发送给服务端的数据; 必须为 bytes 类型
        - headers: a dict; 请求头; 也可通过 `add_header()` 方法添加
        - origin_req_host: 请求方的 host 名称或者 IP 地址
        - unverifiable: 请求是否是无法验证的
        - method: 请求使用的方法

urllib.request.BaseHandler
    Handlers 是对应各种复杂操作的处理器, BaseHandler 类是所有其他 Handler 的父类, 提供了最基本的方法

urllib.request.OpenerDirector
    实现更加底层的操作

.. admonition:: 例

    - **验证**

        网站在打开时就会弹出提示框, 提示输入用户名和密码, 验证成功后才能查看页面

        .. code-block:: python

            from urllib.request import HTTPPasswordMgrWithDefaultRealm, HTTPBasicAuthHandler, build_opener
            from urllib.error import URLError

            username = 'username'
            password = 'password'
            url = 'http://test.com'

            p = HTTPPasswordMgrWithDefaultRealm()
            # 添加用户名和密码
            p.add_password(None, url, username, password)
            # 实例化
            auth_handler = HTTPBasicAuthHandler(p)
            # 构建 Opener
            opener = build_opener(auth_handler)

            try:
                result = opener.open(url)
                html = result.read().decode('utf-8')
                print(html)
            except URLError as e:
                print(e.reason)

    - **代理**

        .. code-block:: python

            from urllib.error import URLError
            from urllib.request import ProxyHandler, build_opener

            # 在本地 9743 端口上搭建了一个代理
            proxy_handler = ProxyHandler({
                'http': 'http://127.0.0.1:9743',
                'https': 'https://127.0.0.1:9743'
            })
            opener = build_opener(proxy_handler)
            try:
                response = opener.open('https://test.com')
                print(response.read().decode('utf-8'))
            except URLError as e:
                print(e.reason)

    - **Cookies**

        获取网站 Cookies:

        .. code-block:: python

            import http.cookiejar, urllib.request

            # 声明一个 CookieJar 对象
            cookie = http.cookiejar.CookieJar()
            # 构建 Handler
            handler = urllib.request.HTTPCookieProcessor(cookie)
            # 构建 Opener
            opener = urllib.request.build_opener(handler)
            response = opener.open('http://test.com')
            for item in cookie:
                print(item.name + '=' + item.value)

        以 Mozilla 浏览器的 Cookies 格式保存:

        .. code-block:: python

            filename = 'cookies.txt'
            cookie = http.cookiejar.MozillaCookieJar(filename)
            handler = urllib.request.HTTPCookieProcessor(cookie)
            opener = urllib.request.build_opener(handler)
            response = opener.open('http://test.com')
            cookie.save(ignore_discard=True, ignore_expires=True)

        以 libwww-perl(LWP) 格式保存:

        .. code-block:: python

            # ...
            cookie = http.cookiejar.LWPCookieJar(filename)
            # ...

        读取 libwww-perl(LWP) 格式的 cookie 文件

        .. code-block:: python

            cookie = http.cookiejar.LWPCookieJar()
            cookie.load('cookies.txt', ignore_discard=True, ignore_expires=True)
            handler = urllib.request.HTTPCookieProcessor(cookie)
            opener = urllib.request.build_opener(handler)
            response = opener.open('http://test.com')
            print(response.read().decode('utf-8'))

error
^^^^^^

异常处理

urllib.error.URLError
    OSError 的子类, 是 error 异常模块的基类, 由 request 模块产生的异常都可以通过捕获这个类来处理

    属性 reason 可以返回错误的原因, 也可能返回一个对象

    .. code-block:: python

        from urllib import request, error
        try:
            response = request.urlopen('https://cuiqingcai.com/index.htm')
        except error.URLError as e:
            print(e.reason)

        # 返回socket.timeout
        import socket

        try:
            response = request.urlopen('https://www.baidu.com', timeout=0.01)
        except error.URLError as e:
            print(type(e.reason))
            if isinstance(e.reason, socket.timeout):
                print('TIME OUT')

urllib.error.HTTPError
    URLError 的子类, 专门用来处理 HTTP 请求错误, 比如认证请求失败等

    属性:

    - `code`: 返回 HTTP 状态码
    - `reason`: 返回错误的原因
    - `headers`: 返回请求头

parse
^^^^^^^

定义了处理 URL 的标准接口, 如实现 URL 各部分的抽取, 合并以及链接转换

支持如下协议的 URL 处理: file, ftp, gopher, hdl, http, https, imap, mailto,  mms, news, nntp, prospero, rsync, rtsp, rtspu, sftp,  sip, sips, snews, svn, svn+ssh, telnet 和 wais

urllib.parse.urlparse(urlstring, scheme='', allow_fragments=True)
    实现 URL 的识别和分段

    PARAMS:
        - urlstring: 待解析的 URL
        - scheme: 默认协议; 假如这个链接没有带协议信息, 会将这个作为默认的协议
        - allow_fragments: 是否忽略 fragment

    .. code-block:: python

        from urllib.parse import urlparse

        result = urlparse('http://www.baidu.com/index.html;user?id=5#comment')
        print(type(result), result)

        # 结果
        # <class 'urllib.parse.ParseResult'> ParseResult(scheme='http', netloc='www.baidu.com', path='/index.html', params='user', query='id=5', fragment='comment')

urllib.parse.urlunparse(parts)
    构造 URL

    parts 是长度必须为 6 的 iterable

urllib.parse.urlsplit(urlstring, scheme='', allow_fragments=True)
    类似于 urlparse, 但不单独解析 params, 只返回 5 个结果

    .. code-block:: python

        from urllib.parse import urlsplit

        result = urlsplit('http://www.baidu.com/index.html;user?id=5#comment')
        print(result)

        # 结果
        # SplitResult(scheme='http', netloc='www.baidu.com', path='/index.html;user', query='id=5', fragment='comment')

urllib.parse.urlunsplit(parts)
    类似于 urlunparse, parts 是长度必须为 5 的 iterable

urllib.parse.urljoin(base, url, allow_fragments=True)
    分析 base_url 的 scheme, netloc 和 path, 并对新链接 url 缺失的部分进行补充, 最后返回结果

urllib.parse.urlencode(query, doseq=False, safe='', encoding=None, errors=None, quote_via=quote_plus)
    多用于构造 GET 参数

    .. code-block:: python

        from urllib.parse import urlencode

        params = {
            'name': 'germey',
            'age': 22
        }
        print(urlencode(params))

        # 结果
        # 'name=germey&age=22'

urllib.parse.parse_qs(qs, keep_blank_values=False, strict_parsing=False, encoding='utf-8', errors='replace', max_num_fields=None)
    解析参数为 dict

    .. code-block:: python

        from urllib.parse import parse_qs

        query = 'name=germey&amp;age=22'
        print(parse_qs(query))

        # 结果
        # {'name': ['germey'], 'age': ['22']}

urllib.parse.parse_qsl(qs, keep_blank_values=False, strict_parsing=False, encoding='utf-8', errors='replace', max_num_fields=None)
    将参数转化为元组组成的列表

    .. code-block:: python

        from urllib.parse import parse_qsl

        query = 'name=germey&amp;age=22'
        print(parse_qsl(query))

        # 结果
        # [('name', 'germey'), ('age', '22')]

urllib.parse.quote(string, safe='/', encoding=None, errors=None)
    将字符串转化为 URL 编码 (ASCII)

urllib.parse.unquote(string, encoding='utf-8', errors='replace')
    将 URL 编码解码为字符串

robots
^^^^^^^^

Robots 协议
    网络爬虫排除标准 (Robots Exclusion Protocol), 也称作爬虫协议, 机器人协议

    用于告诉爬虫和搜索引擎哪些页面可以抓取, 哪些不可以抓取; 它通常是一个叫作 robots.txt 的文本文件, 一般放在网站的根目录下

    当搜索爬虫访问一个站点时, 它首先会检查这个站点根目录下是否存在 robots.txt 文件, 如果存在, 搜索爬虫会根据其中定义的爬取范围来爬取; 如果没有找到这个文件, 搜索爬虫便会访问所有可直接访问的页面

    .. admonition:: 例

        对所有爬虫只允许爬取 public 目录:

        .. code-block:: console

            User-agent: *
            Disallow: /
            Allow: /public/

        禁止所有爬虫访问任何目录:

        .. code-block:: console

            User-agent: *
            Disallow: /

        允许所有爬虫访问任何目录:

        .. code-block:: console

            User-agent: *
            Disallow:

        只允许某一个爬虫访问:

        .. code-block:: console

            User-agent: WebCrawler
            Disallow:
            User-agent: *
            Disallow: /

urllib.robotparser.RobotFileParser(url='')
    根据网站的 robots.txt 文件来判断一个爬虫是否有权限来爬取这个网页

    常用方法:

    - set_url: 用来设置 robots.txt 文件的链接; 如果在创建 RobotFileParser 对象时传入了链接, 就不需要使用这个方法设置
    - read: 读取 robots.txt 文件并进行分析; 这个方法执行读取和分析操作, 如果不调用这个方法, 接下来的判断都会为 False; 不返回任何内容, 但是执行了读取操作
    - parse: 用来解析 robots.txt 文件, 传入的参数是 robots.txt 某些行的内容, 它会按照 robots.txt 的语法规则来分析这些内容
    - can_fetch: 该方法传入两个参数, 第一个是 User-agent, 第二个是要抓取的 URL; 返回的内容是该搜索引擎是否可以抓取这个 URL, 返回结果是 True 或 False
    - mtime: 返回的是上次抓取和分析 robots.txt 的时间, 这对于长时间分析和抓取的搜索爬虫是很有必要的, 可能需要定期检查来抓取最新的 robots.txt
    - modified: 同样对长时间分析和抓取的搜索爬虫很有帮助, 将当前时间设置为上次抓取和分析 robots.txt 的时间


requests
~~~~~~~~~~

GET
^^^^^


Scrapy
--------

Scrapy 是一个基于 Twisted 的异步处理框架, 是纯 Python 实现的爬虫框架

其架构清晰, 模块之间的耦合程度低, 可扩展性极强, 可以灵活完成各种需求

只需要定制开发几个模块就可以轻松实现一个爬虫

Intro
~~~~~~~~

架构
^^^^^^^

.. image:: imgs/scrapy_architecture.jpg

- **Engine**: 引擎, 用来处理整个系统的数据流处理, 触发事务, 是整个框架的核心
- **Item**: 项目, 它定义了爬取结果的数据结构, 爬取的数据会被赋值成该对象
- **Scheduler**: 调度器, 用来接受引擎发过来的请求并加入队列中, 并在引擎再次请求的时候提供给引擎
- **Downloader**: 下载器, 用于下载网页内容, 并将网页内容返回给蜘蛛
- **Spiders**: 蜘蛛, 其内定义了爬取的逻辑和网页的解析规则, 它主要负责解析响应并生成提取结果和新的请求
- **Item Pipeline**: 项目管道, 负责处理由蜘蛛从网页中抽取的项目, 它的主要任务是清洗, 验证和存储数据
- **Downloader Middlewares**: 下载器中间件, 位于引擎和下载器之间的钩子框架, 主要是处理引擎与下载器之间的请求及响应
- **Spider Middlewares**: 蜘蛛中间件, 位于引擎和蜘蛛之间的钩子框架, 主要工作是处理蜘蛛输入的响应和输出的结果及新的请求

数据流
^^^^^^^^^

Scrapy 中的数据流由引擎控制, 其过程如下:

1. Engine 首先打开一个网站, 找到处理该网站的 Spider 并向该 Spider 请求第一个要爬取的 URL
2. Engine 从 Spider 中获取到第一个要爬取的 URL 并通过 Scheduler 以 Request 的形式调度
3. Engine 向 Scheduler 请求下一个要爬取的 URL
4. Scheduler 返回下一个要爬取的 URL 给 Engine, Engine 将 URL 通过 Downloader Middlewares 转发给 Downloader 下载
5. 一旦页面下载完毕,  Downloader 生成一个该页面的 Response, 并将其通过 Downloader Middlewares 发送给 Engine
6. Engine 从下载器中接收到 Response 并通过 Spider Middlewares 发送给 Spider 处理
7. Spider 处理 Response 并返回爬取到的 Item 及新的 Request 给 Engine
8. Engine 将 Spider 返回的 Item 给 Item Pipeline, 将新的 Request 给 Scheduler
9. 重复第二步到最后一步, 直到  Scheduler 中没有更多的 Request, Engine 关闭该网站, 爬取结束

通过多个组件的相互协作, 不同组件完成工作的不同, 组件对异步处理的支持, Scrapy 最大限度地利用了网络带宽, 大大提高了数据爬取和处理的效率

项目结构
^^^^^^^^^^^

Scrapy 通过命令行来创建项目, 项目创建之后文件结构如下:

.. code-block:: console

    scrapy.cfg
    project/
        __init__.py
        items.py
        pipelines.py
        settings.py
        middlewares.py
        spiders/
            __init__.py
            spider1.py
            spider2.py
            ...

- **scrapy.cfg**: Scrapy 项目的配置文件, 其内定义了项目的配置文件路径, 部署相关信息等内容
- **items.py**: 定义 Item 数据结构, 所有的 Item 的定义都可以放这里
- **pipelines.py**: 定义 Item Pipeline 的实现, 所有的 Item Pipeline 的实现都可以放这里
- **settings.py**: 定义项目的全局配置
- **middlewares.py**: 定义 Spider Middlewares 和 Downloader Middlewares 的实现
- **spiders**: 其内包含一个个 Spider 的实现, 每个 Spider 都有一个文件

Scrapy 使用
~~~~~~~~~~~~~~~

1. 创建项目

    .. code-block:: console

        $ scrapy startproject projectname

    生成项目文件夹

    .. code-block:: console

        scrapy.cfg          # Scrapy 部署时的配置文件
        projectname         # 项目的模块, 引入的时候需要从这里引入
            __init__.py
            items.py        # Items 的定义, 定义爬取的数据结构
            middlewares.py  # Middlewares 的定义, 定义爬取时的中间件
            pipelines.py    # Pipelines 的定义, 定义数据管道
            settings.py     # 配置文件
            spiders         # 放置 Spiders 的文件夹
                __init__.py

2. 创建 spider

    子类化 `scrapy.Spider`, 定义 Spider 的名称和起始请求, 以及处理爬取结果的方法

    或者在命令行创建:

    .. code-block:: console

        $ scrapy genspider spider1

    以上命令会在 spiders 文件夹中创建 `spider1.py`:

    .. code-block:: python

        # spider1.py

        import scrapy

        class Spider1Spider(scrapy.Spider):
            name = "spider1"
            allowed_domains = ["spider1.toscrape.com"]
            start_urls = ['http://spider1.toscrape.com/']

            def parse(self, response):
                pass

    - **name**: 每个项目唯一的名字, 用来区分不同的 Spider
    - **allowed_domains**: 允许爬取的域名, 如果初始或后续的请求链接不是这个域名下的, 则请求链接会被过滤掉
    - **start_urls**: 包含了 Spider 在启动时爬取的 url 列表, 初始请求是由它来定义的
    - **parse(response)**: Spider 的一个方法; 默认情况下, 被调用时 start_urls 里面的链接构成的请求完成下载执行后, 返回的响应就会作为唯一的参数传递给这个函数; 该方法负责解析返回的响应, 提取数据或者进一步生成要处理的请求

3. 创建 Item

    子类化 `scrapy.Item`, 定义类型为 `scrapy.Field` 的字段来创建保存爬取数据的容器; 和字典类似

    .. code-block:: python

        import scrapy

        class Spider1Item(scrapy.Item):

            text = scrapy.Field()
            author = scrapy.Field()
            tags = scrapy.Field()
            # ...

4. 在 `parse()` 中解析 Response

5. 在 `parse()` 中实例化自定义 Item 类

6. 在 `parse()` 中创建下一个请求

    实例化 `scrapy.Request(url, callback)` 并 `yield`

    - url：请求链接
    - callback：回调函数; 当指定了该回调函数的请求完成之后, 获取到响应, 引擎会将该响应作为参数传递给这个回调函数; 回调函数进行解析或生成下一个请求

7. 在命令行运行定义的 spider

    .. code-block:: console

        $ scrapy crawl spider1

8. 保存抓取结果到文件

    .. code-block:: console

        $ scrapy crawl quotes -o results.json

    支持 json, jsonline, csv, xml, pickle, marshal 等格式, 还支持 ftp, s3 等远程输出, 也可自定义 `ItemExporter` 来实现其他输出

9. 使用 Item Pipeline 进行更复杂的操作

    定义实现了 `process_item(item, spider)` 方法的类即可; 启用 Item Pipeline 后, Item Pipeline 会自动调用这个方法; `process_item()` 方法必须返回包含数据的字典或 Item 对象, 或者抛出 `DropItem` 异常

    其他可定义的方法:

    - **from_crawler(cls, crawler)**: `@classmethod`, 通过 crawler 可以获取 settings.py 中的设置, 如数据库连接地址和名称等
    - **open_spider(self, spider)**: 当 Spider 被开启时调用, 主要进行一些初始化操作
    - **close_spider(self, spider)**: 当 Spider 被关闭时调用, 进行清理工作

    能够实现如

    - 清洗 HTML 数据
    - 验证爬取数据, 检查爬取字段
    - 查重并丢弃重复内容
    - 将爬取结果储存到数据库

    等操作

Selector
~~~~~~~~~~~

基于 lxml, 支持 XPath 选择器, CSS 选择器以及正则表达式

.. code-block:: python

    from scrapy import Selector

selector 对象可以使用 `xpath()` 或 `css()` 获取元素

创建 selector
^^^^^^^^^^^^^^^^^^

- `response` 对象的 `selector` 属性是一个 `Selector` 的实例

    .. code-block:: python

        response.selector.xpath('//span/text()').get()

- 也可以通过传入 HTML 或 response 实例化 `Selector`

    .. code-block:: python

        from scrapy.selector import Selector

        body = '<html><body><span>good</span></body></html>'

        Selector(text=body).xpath('//span/text()').get()

`response.selector.xpath()` 或 `response.selector.css()` 可简写为 `response.xpath()` 或 `response.css()`

scrapy.selector.Selector(response=None, text=None, type=None, root=None, **kwargs)
    Selector 的实例是 response 的一个 wrapper, 用于选择其内容的一部分

    - `response`: `HtmlResponse` 或 `XmlResponse` 对象

    - `text`: unicode 字符串或 utf-8 编码的文本; 不能与 response 一起使用

    - `type`: selector 类型; 可以是"html", "xml" 或 None (default)
        `type` 为 None 时, 传入 `text` 则默认为 "html"; 传入 `response` 则根据 `response` 类型决定

        - `HtmlResponse` -> "html"
        - `XmlResponse` -> "xml"
        - 其他 -> "html"

        当 `type` 被设置时, 指定的类型会被强制使用

    - `xpath(query, namespaces=None, **kwargs)`
        寻找符合 xpath query 的 nodes 并返回含有所有元素的 `SelectorList` 实例

        - query: 包含 XPATH query 的字符串
        - namespaces: 可选的 `prefix: namespace-uri`

    - `css(query)`
        寻找符合 CSS query 的 nodes 并返回含有所有元素的 `SelectorList` 实例

        - query: 包含 CSS query 的字符串

            CSS queries 会被转换成 XPath queries (使用 cssselect 库) 并调用 `.xpath()` 方法

    - `get()`
        序列化并返回匹配的 nodes 为 unicode 字符串; 百分号编码 (Percent encoded) 的内容会被解码

    - attrib
        返回元素的属性字典

    - `re(regex, replace_entities=True)`
        返回匹配正则表达式的 unicode 字符串的列表

        - regex: 已编译的正则表达式或能够被 `re.compile(regex)` 编译的字符串

        默认情况下, 除了 `&amp;` 和 `&lt;` 以外的字符实体引用会被替换为对应的字符; 设置 `replace_entities=False` 可以关闭替换

    - `re_first(regex, default=None, replace_entities=True)`
        返回匹配正则表达式的 unicode 字符串的而列表的第一个字符串; 如果没有匹配则返回默认值

        默认情况下, 除了 `&amp;` 和 `&lt;` 以外的字符实体引用会被替换为对应的字符; 设置 `replace_entities=False` 可以关闭替换

    - `register_namespace(prefix, uri)`
        Register the given namespace to be used in this Selector. Without registering namespaces you can’t select or extract data from non-standard namespaces

    - `remove_namespaces()`
        Remove all namespaces, allowing to traverse the document using namespace-less xpaths

    - `__bool__()`
        Return True if there is any real content selected or False otherwise. In other words, the boolean value of a Selector is given by the contents it selects.

    - `getall()`
        返回只含有 `get()` 的返回值的列表

使用 selector 获取结果
^^^^^^^^^^^^^^^^^^^^^^^^^

`xpath()` 和 `css()` 返回 `SelectorList` 实例, 它是由新的 selectors 组成的列表

scrapy.selector.SelectorList
    内置 `list` 类的子类

    - `xpath(xpath, namespaces=None, **kwargs)`
        调用列表中每个元素的 `.xpath()` 方法并返回结果为新的 `SelectorList`

        - xpath: 包含 XPATH query 的字符串
        - namespaces: 可选的 `prefix: namespace-uri`

    - `css(query)`
        调用列表中每个元素的 `.css()` 方法并返回结果为新的 `SelectorList`

        - query: 包含 CSS query 的字符串

    - `getall()`
        调用列表中每个元素的 `.css()` 方法并返回结果为 unicode 字符串的列表

    - `get(default=None)`
        返回列表中第一个元素的 `.get()` 方法的结果

        若列表为空则返回默认值

    - `re(regex, replace_entities=True)`
        调用列表中每个元素的 `.re()` 方法并返回结果为 unicode 字符串的列表

        默认情况下, 除了 `&amp;` 和 `&lt;` 以外的字符实体引用会被替换为对应的字符; 设置 `replace_entities=False` 可以关闭替换

    - `re_first(regex, default=None, replace_entities=True)`
        返回列表中第一个元素的 `.re()` 方法的结果

        若列表为空或没有匹配则返回默认值

        默认情况下, 除了 `&amp;` 和 `&lt;` 以外的字符实体引用会被替换为对应的字符; 设置 `replace_entities=False` 可以关闭替换

    - attrib
        返回列表中第一个元素的属性字典

        若列表为空或没有匹配则返回空字典

CSS 选择器扩展
^^^^^^^^^^^^^^^^

原生 CSS 选择器无法选择 text nodes 或属性值

- `::text`: 选择文本
- `::attr(name)`: 选择指定属性名

Spider
~~~~~~~~

- 定义爬取网站的动作
- 分析爬取下来的网页

Spider 的工作流程:

1. 以初始的 URL 初始化 Request, 并设置回调函数; 当该 Request 成功请求并返回时, 将生成 Response, 并作为参数传给该回调函数

2. 在回调函数内分析返回的网页内容; 回调函数的返回值为包含数据的 dicts, Item 对象, Request 对象, 或包含了这些对象的可迭代对象; 如果返回的是 Reqeust, 那么 Request 执行成功得到 Response 之后会再次传递给 Request 中定义的回调函数, 可以再次使用选择器来分析新得到的网页

3. 在回调函数中解析网页内容并用解析的数据生成 items

4. 最后如果返回的是字典或 Item 对象, 可通过 Feed Exports 等形式存入到文件, 如果设置了 Pipeline 的话, 可以经由 Pipeline 处理（如过滤, 修正等) 并保存

scrapy.spiders.Spider
    这个类是最简单最基本的 Spider 类, 每个其他的 Spider 必须继承这个类

    这个类里提供了 start_requests() 方法的默认实现, 读取并请求 start_urls 属性, 并根据返回的结果调用 parse() 方法解析结果

    - name
        爬虫名称, 定义 Spider 名字的字符串; Spider 的名字定义了 Scrapy 如何定位并初始化 Spider, 所以其必须是唯一的

        name 是 Spider 最重要的属性, 而且是必须的

        如果该 Spider 爬取单个网站, 通常以该网站的域名名称来命名 Spider

    - allowed_domains
        允许爬取的域名列表, 可选; 不在此范围的链接不会被跟进爬取

    - start_urls
        起始 URL 列表, 当没有实现 start_requests() 方法时, 默认会从这个列表开始抓取

        随后的 `Request` 会由 start_urls 中的 data 生成

    - custom_settings
        这是一个字典, 是专属于本 Spider 的配置, 此设置会覆盖项目全局的设置; 且此设置必须在初始化前被更新, 所以它必须定义成类变量

    - crawler
        此属性在类初始化后由 `from_crawler()` 方法设置, 代表的是本 Spider 类对应的 `Crawler` 对象; `Crawler` 对象中包含了很多项目组件, 利用它可以获取项目的一些配置信息, 如最常见的就是获取项目的设置信息, 即 Settings

    - settings
        `Settings` 对象, 可以直接获取项目的全局设置变量

    - logger
        用 Spider 名创建的 `logger`

    - `from_crawler(crawler, *args, **kwargs)`
        Scrapy 用于创建 spiders 的类方法

        You probably won’t need to override this directly because the default implementation acts as a proxy to the __init__() method, calling it with the given arguments args and named arguments kwargs.

        Nonetheless, this method sets the crawler and settings attributes in the new instance so they can be accessed later inside the spider’s code.

        - crawler (Crawler instance): crawler to which the spider will be bound

    - start_requests()
        用于生成初始请求, 必须返回一个可迭代对象, 当开始爬取时由 Scrapy 调用, 且只被调用一次; 默认使用 start_urls 里面的 URL 来构造 GET 请求的 Request(url, dont_filter=True); 如果要在启动时以改变访问方式, 可以重写这个方法

    - parse(response)
        当 requests 没有指定回调函数时, 该方法会默认被调用, 它负责处理 Response, 处理返回结果, 并从中提取出想要的数据和下一步的请求, 然后返回; 该方法需要返回一个包含 Request 或 Item 的可迭代对象

    - log(message[, level, component])
        发送 log 信息到 Spider 的 logger

    - closed(reason)
        当 Spider 关闭时, 该方法会被调用, 在这里一般会定义释放资源的一些操作或其他收尾操作

通用 Spiders
~~~~~~~~~~~~~~~

CrawlSpider
^^^^^^^^^^^^^

XMLFeedSpider
^^^^^^^^^^^^^^^

CSVFeedSpider
^^^^^^^^^^^^^^^

SitemapSpider
^^^^^^^^^^^^^^^

Downloader Middleware
~~~~~~~~~~~~~~~~~~~~~~

下载器中间件, 处于 Request 和 Response 之间的处理模块

- 在 Scheduler 调度出队列的 Request 发送给 Downloader 下载之前, 处理 requests
- 在下载后生成的 Response 发送给 Spider 之前, 处理 responses

Scrapy 预置了许多 Downloader Middleware, 定义于 `DOWNLOADER_MIDDLEWARES_BASE` 变量中:

.. code-block:: python

    # key 为 Downloader Middleware 名
    # value 为调用的优先级, 也代表靠近 Scrapy 引擎的程度

    {
        'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware': 100,
        'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware': 300,
        'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware': 350,
        'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware': 400,
        'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': 500,
        'scrapy.downloadermiddlewares.retry.RetryMiddleware': 550,
        'scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware': 560,
        'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware': 580,
        'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 590,
        'scrapy.downloadermiddlewares.redirect.RedirectMiddleware': 600,
        'scrapy.downloadermiddlewares.cookies.CookiesMiddleware': 700,
        'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 750,
        'scrapy.downloadermiddlewares.stats.DownloaderStats': 850,
        'scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware': 900,
    }

每个 Downloader Middleware 都可以定义 `process_request()` 和 `process_response()` 方法来分别处理请求和响应:

- 对于 process_request(), 优先级数字越小越先被调用
- 对于 process_response(), 优先级数字越大越先被调用

自定义 Downloader Middleware 时, 不能直接修改 `DOWNLOADER_MIDDLEWARES_BASE`; 可以通过修改 `DOWNLOADER_MIDDLEWARES` 变量添加自定义 Downloader Middleware, 或禁用 `DOWNLOADER_MIDDLEWARES_BASE` 中的 Downloader Middlewares

自定义 Downloader Middleware
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

只需定义以下 3 个核心方法中的 1 个即可:

- `process_request(request, spider)`
    Request 被 Scrapy 引擎调度给 Downloader 之前, process_request() 方法就会被调用, 也就是在 Request 从队列里调度出来到 Downloader 下载执行之前都可以用 process_request() 方法对 Request 进行处

    方法的返回值必须为 None, Response 对象, Request 对象之一, 或者抛出 `IgnoreRequest` 异常

    - request: Request 对象, 即被处理的 Request
    - spider: Spdier 对象, 即此 Request 对应的 Spider

    **返回类型**:

    - None

        Scrapy 将继续处理该 Request, 接着执行其他 Downloader Middleware 的 process_request() 方法,

        直到 Downloader 把 Request 执行后得到 Response 才结束

        这个过程其实就是修改 Request 的过程; 不同的 Downloader Middleware 按照设置的优先级顺序依次对 Request 进行修改, 最后送至 Downloader 执行

    - Response 对象

        更低优先级的 Downloader Middleware 的 process_request() 和 process_exception() 方法不会被继续调用; 每个 Downloader Middleware 的 process_response() 方法转而被依次调用

        调用完毕之后, 直接将 Response 对象发送给 Spider 来处理

    - Request 对象

        更低优先级的 Downloader Middleware 的 process_request() 方法会停止执行; 这个 Request 会重新放到调度队列里, 其实它就是一个全新的 Request, 等待被调度

        如果被 Scheduler 调度了, 那么所有的 Downloader Middleware 的 process_request() 方法会被重新按照顺序执行

    - IgnoreRequest 异常

        所有的 Downloader Middleware 的 process_exception() 方法会依次执行

        如果没有一个方法处理这个异常, 那么 Request 的 errorback() 方法就会回调

        如果该异常还没有被处理, 那么它便会被忽略

- `process_response(request, response, spider)`
    Downloader 执行 Request 下载之后, 会得到对应的 Response; Scrapy 引擎便会将 Response 发送给 Spider 进行解析

    在发送之前, 可以用 process_response() 方法来对 Response 进行处理

    方法的返回值必须为 Request 对象, Response 对象之一, 或者抛出 IgnoreRequest 异常

    - request: Request 对象, 即此 Response 对应的 Request
    - response: Response 对象, 即被处理的 Response
    - spider: Spider 对象, 即此 Response 对应的 Spider

    **返回类型**:

    - Request 对象

        更低优先级的 Downloader Middleware 的 process_response() 方法不会继续调用

        该 Request 对象会重新放到调度队列里等待被调度, 它相当于一个全新的 Request

        然后该 Request 会被 process_request() 方法顺次处理

    - Response 对象

        更低优先级的 Downloader Middleware 的 process_response() 方法会继续调用, 继续对该 Response 对象进行处理

    - IgnoreRequest 异常

        Request 的 errorback() 方法会回调

        如果该异常还没有被处理, 那么便会被忽略

- `process_exception(request, exception, spider)`
    当 Downloader 或 process_request() 方法抛出异常时, process_exception() 方法就会被调用

    方法的返回值必须为 None, Response 对象, Request 对象之一

    - request: Request 对象, 即产生异常的 Request
    - exception: Exception 对象, 即抛出的异常
    - spdier: Spider 对象, 即 Request 对应的 Spider

    **返回类型**:

    - None

        更低优先级的 Downloader Middleware 的 process_exception() 会被继续顺次调用, 直到所有的方法都被调度完毕

    - Response 对象

        更低优先级的 Downloader Middleware 的 process_exception() 方法不再被继续调用, 每个 Downloader Middleware 的 process_response() 方法转而被依次调用

    - Request 对象

        更低优先级的 Downloader Middleware 的 process_exception() 不再被继续调用, 该 Request 对象会重新放到调度队列里面等待被调度, 它相当于一个全新的 Request; 然后该 Request 又会被 process_request() 方法顺次处理

